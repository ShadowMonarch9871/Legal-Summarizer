{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShadowMonarch9871/Legal-Summarizer/blob/main/Legal%20Summarizer%20using%20LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QmES3vZZWwnU",
        "outputId": "79e6081e-30c5-41f1-8af9-7d1d156c25b5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.29.0)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.10.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.30.2)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.17)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.8)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.55 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.56)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.38)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (9.1.2)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.55->langchain) (1.33)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.1)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.55->langchain) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.23-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.56 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.56)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.24 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.24)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.38)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.24->langchain-community) (2.11.3)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.56->langchain-community) (4.13.2)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.17)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.4.26)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.56->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.24->langchain-community) (2.33.1)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.23-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m34.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.23 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gradio langchain transformers torch pypdf\n",
        "!pip install -U langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "HFUEq1V_pyuz",
        "outputId": "d06a43db-0da2-4451-cc22-f588bc4a6129"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://dbf2265ca7775bd30e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://dbf2265ca7775bd30e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://dbf2265ca7775bd30e.gradio.live\n"
          ]
        }
      ],
      "source": [
        "import gradio as gr\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "import torch\n",
        "import re\n",
        "import tempfile\n",
        "import nltk\n",
        "try:\n",
        "    from nltk.tokenize import sent_tokenize\n",
        "except ImportError:\n",
        "    print(\"NLTK import failed, will use custom sentence tokenizer\")\n",
        "\n",
        "# Download NLTK resources (only needed once)\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except LookupError:\n",
        "    try:\n",
        "        nltk.download('punkt', quiet=True)\n",
        "        print(\"Successfully downloaded NLTK punkt resource\")\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Could not download NLTK resources: {e}\")\n",
        "        print(\"Will use custom sentence splitting instead\")\n",
        "\n",
        "# Constants\n",
        "MAX_CHUNK_LENGTH = 1024  # Token limit for T5-based models\n",
        "MODEL_NAME = \"manjunathainti/fine_tuned_t5_summarizer\"  # Pretrained Legal T5 summarizer\n",
        "SHORT_SUMMARY_LENGTH = 150  # For concise summaries\n",
        "LONG_SUMMARY_LENGTH = 300  # For detailed summaries\n",
        "\n",
        "# Load the T5 model and tokenizer\n",
        "def load_model():\n",
        "    \"\"\"Load a T5 model fine-tuned for legal summarization.\"\"\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Using device: {device}\")\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(device)\n",
        "    return tokenizer, model\n",
        "\n",
        "# Clean and preprocess the input text\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Clean and standardize extracted legal text.\"\"\"\n",
        "    # Remove page numbers, dates, and footnotes\n",
        "    text = re.sub(r\"Page \\d+|[0-9]{1,2}/[0-9]{1,2}/[0-9]{2,4}|Footnote.*\", \"\", text)\n",
        "\n",
        "    # Handle section markers more carefully\n",
        "    text = re.sub(r\"^\\s*\\([a-z]\\)\\s*\", \"\\n\\\\0\", text, flags=re.MULTILINE)\n",
        "\n",
        "    # Normalize whitespace\n",
        "    text = re.sub(r\"\\n{3,}\", \"\\n\\n\", text)  # Standardize paragraph breaks\n",
        "    text = re.sub(r\"\\s{2,}\", \" \", text)     # Remove multiple spaces\n",
        "\n",
        "    # Replace dashed separators with paragraph breaks\n",
        "    text = re.sub(r\"[*\\-_]{3,}\", \"\\n\\n\", text)\n",
        "\n",
        "    # Handle common legal document formatting issues\n",
        "    text = re.sub(r\"(?<=[a-z])\\.\\s*(?=[A-Z])\", \".\\n\", text)  # Add line break between sentences if missing\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "# Intelligently split text into semantic chunks\n",
        "def chunk_text(text, max_tokens=MAX_CHUNK_LENGTH, overlap=150):\n",
        "    \"\"\"Split text into smaller chunks that respect sentence boundaries when possible.\"\"\"\n",
        "    # First split by paragraphs\n",
        "    paragraphs = text.split(\"\\n\\n\")\n",
        "    chunks = []\n",
        "\n",
        "    # Create a splitter with higher overlap to maintain context between chunks\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=max_tokens,\n",
        "        chunk_overlap=overlap,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \". \", \", \", \" \", \"\"]\n",
        "    )\n",
        "\n",
        "    current_chunk = \"\"\n",
        "    for para in paragraphs:\n",
        "        # If adding this paragraph would exceed the limit, process the current chunk\n",
        "        if len(current_chunk) + len(para) > max_tokens * 4:  # Approximate char count\n",
        "            # Further split if needed\n",
        "            if len(current_chunk) > max_tokens * 4:\n",
        "                sub_chunks = splitter.split_text(current_chunk)\n",
        "                chunks.extend(sub_chunks)\n",
        "            else:\n",
        "                chunks.append(current_chunk)\n",
        "            current_chunk = para\n",
        "        else:\n",
        "            if current_chunk:\n",
        "                current_chunk += \"\\n\\n\" + para\n",
        "            else:\n",
        "                current_chunk = para\n",
        "\n",
        "    # Add the last chunk\n",
        "    if current_chunk:\n",
        "        if len(current_chunk) > max_tokens * 4:\n",
        "            sub_chunks = splitter.split_text(current_chunk)\n",
        "            chunks.extend(sub_chunks)\n",
        "        else:\n",
        "            chunks.append(current_chunk)\n",
        "\n",
        "    # Ensure chunks aren't too small\n",
        "    filtered_chunks = [c for c in chunks if len(c.split()) > 20]\n",
        "\n",
        "    return filtered_chunks\n",
        "\n",
        "# Generate both short and long summaries for text chunks\n",
        "def summarize_chunks(chunks, tokenizer, model, summary_type=\"both\", num_beams=4):\n",
        "    \"\"\"Generate summaries for each chunk of text with options for short or long summaries.\"\"\"\n",
        "    device = next(model.parameters()).device\n",
        "    short_summaries = []\n",
        "    long_summaries = []\n",
        "\n",
        "    for chunk in chunks:\n",
        "        inputs = tokenizer(chunk, return_tensors=\"pt\", truncation=True, max_length=MAX_CHUNK_LENGTH).to(device)\n",
        "\n",
        "        # Generate short summary\n",
        "        if summary_type in [\"short\", \"both\"]:\n",
        "            short_summary_ids = model.generate(\n",
        "                inputs.input_ids,\n",
        "                max_length=SHORT_SUMMARY_LENGTH,\n",
        "                min_length=min(50, len(chunk.split()) // 10),  # Reasonable minimum length\n",
        "                num_beams=num_beams,\n",
        "                early_stopping=True,\n",
        "                length_penalty=1.0,\n",
        "                no_repeat_ngram_size=3,\n",
        "            )\n",
        "            short_summary = tokenizer.decode(short_summary_ids[0], skip_special_tokens=True)\n",
        "            short_summaries.append(short_summary.strip())\n",
        "\n",
        "        # Generate long summary\n",
        "        if summary_type in [\"long\", \"both\"]:\n",
        "            long_summary_ids = model.generate(\n",
        "                inputs.input_ids,\n",
        "                max_length=LONG_SUMMARY_LENGTH,\n",
        "                min_length=min(100, len(chunk.split()) // 5),  # Reasonable minimum length\n",
        "                num_beams=num_beams,\n",
        "                early_stopping=True,\n",
        "                length_penalty=1.0,\n",
        "                no_repeat_ngram_size=2,\n",
        "            )\n",
        "            long_summary = tokenizer.decode(long_summary_ids[0], skip_special_tokens=True)\n",
        "            long_summaries.append(long_summary.strip())\n",
        "\n",
        "    return short_summaries, long_summaries\n",
        "\n",
        "# Custom sentence tokenizer as fallback if NLTK is not available\n",
        "def custom_sent_tokenize(text):\n",
        "    \"\"\"Split text into sentences using regex patterns.\"\"\"\n",
        "    # Split on common sentence endings followed by space and capital letter\n",
        "    sentences = re.split(r'(?<=[.!?])\\s+(?=[A-Z])', text)\n",
        "\n",
        "    # Further split any remaining long segments\n",
        "    result = []\n",
        "    for sent in sentences:\n",
        "        if len(sent) > 150:  # If sentence is too long\n",
        "            subsents = re.split(r'(?<=[;:])\\s+(?=[A-Z])', sent)  # Split on semicolons and colons\n",
        "            result.extend(subsents)\n",
        "        else:\n",
        "            result.append(sent)\n",
        "    return result\n",
        "\n",
        "# Intelligently combine summaries for coherence\n",
        "def combine_summaries(summaries, max_length=None):\n",
        "    \"\"\"Combine summaries into a coherent text, avoiding redundancy.\"\"\"\n",
        "    if not summaries:\n",
        "        return \"\"\n",
        "\n",
        "    # Extract unique sentences to avoid redundancy\n",
        "    all_sentences = []\n",
        "    sentence_set = set()\n",
        "\n",
        "    for summary in summaries:\n",
        "        # Try to use NLTK's tokenizer, fall back to custom if not available\n",
        "        try:\n",
        "            sentences = sent_tokenize(summary)\n",
        "        except (NameError, LookupError):\n",
        "            sentences = custom_sent_tokenize(summary)\n",
        "\n",
        "        for sentence in sentences:\n",
        "            # Normalize sentence for comparison\n",
        "            normalized = re.sub(r'\\s+', ' ', sentence.lower()).strip()\n",
        "            if normalized not in sentence_set and len(normalized) > 10:\n",
        "                sentence_set.add(normalized)\n",
        "                all_sentences.append(sentence)\n",
        "\n",
        "    # If we have too many sentences, prioritize the first sentence from each summary\n",
        "    # to maintain coverage of the entire document\n",
        "    if max_length and len(' '.join(all_sentences)) > max_length:\n",
        "        first_sentences = []\n",
        "        for summary in summaries:\n",
        "            try:\n",
        "                sentences = sent_tokenize(summary)\n",
        "            except (NameError, LookupError):\n",
        "                sentences = custom_sent_tokenize(summary)\n",
        "\n",
        "            if sentences:\n",
        "                first_sentence = sentences[0]\n",
        "                normalized = re.sub(r'\\s+', ' ', first_sentence.lower()).strip()\n",
        "                if normalized not in sentence_set and len(normalized) > 10:\n",
        "                    sentence_set.add(normalized)\n",
        "                    first_sentences.append(first_sentence)\n",
        "\n",
        "        # Use first sentences if they're representative enough\n",
        "        if first_sentences and len(' '.join(first_sentences)) >= max_length // 2:\n",
        "            all_sentences = first_sentences\n",
        "\n",
        "    # Join sentences, ensuring proper spacing\n",
        "    combined_text = ' '.join(all_sentences)\n",
        "\n",
        "    # Clean up spacing and formatting\n",
        "    combined_text = re.sub(r'\\s+', ' ', combined_text)\n",
        "    combined_text = re.sub(r'\\s+\\.', '.', combined_text)\n",
        "    combined_text = re.sub(r'\\s+,', ',', combined_text)\n",
        "\n",
        "    return combined_text\n",
        "\n",
        "# Extract legal terms from the document for improved highlighting\n",
        "def extract_legal_terms(text):\n",
        "    \"\"\"Extract domain-specific legal terms from the document.\"\"\"\n",
        "    base_legal_terms = [\n",
        "        \"Section\", \"Article\", \"Clause\", \"Amendment\", \"Schedule\", \"Act\", \"Law\",\n",
        "        \"Constitution\", \"Provision\", \"Regulation\", \"Statute\", \"Directive\",\n",
        "        \"Legislative\", \"Assembly\", \"Parliament\", \"Election\", \"Commission\",\n",
        "        \"President\", \"Governor\", \"Cabinet\", \"Council\", \"Minister\", \"Bill\"\n",
        "    ]\n",
        "\n",
        "    # Find potential additional terms (capitalized multi-word phrases)\n",
        "    additional_terms = re.findall(r'\\b([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)+)\\b', text)\n",
        "\n",
        "    # Combine and remove duplicates\n",
        "    all_terms = base_legal_terms + additional_terms\n",
        "    return list(set(all_terms))\n",
        "\n",
        "# Highlight keywords in the summary\n",
        "def highlight_keywords(summary_text, keywords=None):\n",
        "    \"\"\"Emphasize critical legal terms in the summary.\"\"\"\n",
        "    if keywords is None:\n",
        "        keywords = [\n",
        "            \"Section\", \"Article\", \"Clause\", \"Amendment\", \"Schedule\", \"Act\", \"Law\",\n",
        "            \"Constitution\", \"Provision\", \"Regulation\"\n",
        "        ]\n",
        "\n",
        "    # Highlight terms that are complete words (not parts of other words)\n",
        "    for keyword in sorted(keywords, key=len, reverse=True):\n",
        "        summary_text = re.sub(\n",
        "            fr'\\b({re.escape(keyword)})\\b',\n",
        "            r'**\\1**',\n",
        "            summary_text,\n",
        "            flags=re.IGNORECASE\n",
        "        )\n",
        "\n",
        "    return summary_text\n",
        "\n",
        "# Summarize sections matching specific keywords\n",
        "def summarize_target_sections(text, tokenizer, model, keywords, summary_type=\"both\", num_beams=4):\n",
        "    \"\"\"Summarize specific sections of the document based on target keywords.\"\"\"\n",
        "    # Create a safe regex pattern from keywords\n",
        "    try:\n",
        "        keyword_regex = \"|\".join(re.escape(keyword) for keyword in keywords if keyword.strip())\n",
        "    except Exception:\n",
        "        # Fallback if there's a problem with the regex\n",
        "        print(\"Warning: Issue with keyword regex, using simple matching\")\n",
        "        keyword_regex = \"|\".join(keywords)\n",
        "\n",
        "    # Extract paragraphs containing keywords\n",
        "    paragraphs = text.split(\"\\n\\n\")\n",
        "    target_sections = []\n",
        "\n",
        "    # Use try/except to handle potential regex issues\n",
        "    try:\n",
        "        target_sections = [\n",
        "            para for para in paragraphs\n",
        "            if para.strip() and any(re.search(re.escape(kw), para, re.IGNORECASE) for kw in keywords)\n",
        "        ]\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Error in keyword matching: {e}\")\n",
        "        # Simple fallback matching\n",
        "        target_sections = [\n",
        "            para for para in paragraphs\n",
        "            if para.strip() and any(kw.lower() in para.lower() for kw in keywords)\n",
        "        ]\n",
        "\n",
        "    if not target_sections:\n",
        "        return \"No sections containing the specified keywords were found.\", \"\"\n",
        "\n",
        "    # Combine related paragraphs to maintain context\n",
        "    combined_sections = []\n",
        "    current_section = \"\"\n",
        "\n",
        "    for section in target_sections:\n",
        "        if not current_section:\n",
        "            current_section = section\n",
        "        elif len(current_section) + len(section) < MAX_CHUNK_LENGTH * 4:\n",
        "            current_section += \"\\n\\n\" + section\n",
        "        else:\n",
        "            combined_sections.append(current_section)\n",
        "            current_section = section\n",
        "\n",
        "    if current_section:\n",
        "        combined_sections.append(current_section)\n",
        "\n",
        "    # Handle empty combined sections (shouldn't happen, but just in case)\n",
        "    if not combined_sections:\n",
        "        return \"Error: No valid sections to summarize.\", \"\"\n",
        "\n",
        "    # Summarize each combined section\n",
        "    short_summaries, long_summaries = summarize_chunks(\n",
        "        combined_sections, tokenizer, model, summary_type, num_beams\n",
        "    )\n",
        "\n",
        "    # Combine summaries with error handling\n",
        "    try:\n",
        "        short_combined = combine_summaries(short_summaries, SHORT_SUMMARY_LENGTH * 2)\n",
        "        long_combined = combine_summaries(long_summaries, LONG_SUMMARY_LENGTH * 2)\n",
        "    except Exception as e:\n",
        "        print(f\"Warning: Error combining summaries: {e}\")\n",
        "        # Simple concatenation as fallback\n",
        "        short_combined = \" \".join(short_summaries)\n",
        "        long_combined = \" \".join(long_summaries)\n",
        "\n",
        "    return short_combined, long_combined\n",
        "\n",
        "# Create a structured summary with sections\n",
        "def create_structured_summary(short_summary, long_summary, metrics, keywords=None):\n",
        "    \"\"\"Format the summary into a structured document with sections.\"\"\"\n",
        "    legal_terms = keywords if keywords else []\n",
        "\n",
        "    # Highlight keywords in both summaries\n",
        "    highlighted_short = highlight_keywords(short_summary, legal_terms)\n",
        "    highlighted_long = highlight_keywords(long_summary, legal_terms)\n",
        "\n",
        "    # Create a structured summary document\n",
        "    structured_summary = f\"\"\"# Legal Document Summary\n",
        "\n",
        "## Metrics\n",
        "{metrics}\n",
        "\n",
        "## Executive Summary\n",
        "{highlighted_short}\n",
        "\n",
        "## Detailed Summary\n",
        "{highlighted_long}\n",
        "\"\"\"\n",
        "    return structured_summary\n",
        "\n",
        "# Process PDF and generate summaries\n",
        "def process_pdf(file_obj, keywords=None, summary_type=\"both\", num_beams=4):\n",
        "    \"\"\"Process a legal PDF document, summarize it, and provide metrics.\"\"\"\n",
        "    if file_obj is None:\n",
        "        return \"Please upload a PDF file.\", None\n",
        "\n",
        "    try:\n",
        "        # Read and preprocess the uploaded PDF\n",
        "        loader = PyPDFLoader(file_obj.name)\n",
        "        pages = loader.load_and_split()\n",
        "        text = \" \".join([page.page_content for page in pages])\n",
        "        text = preprocess_text(text)\n",
        "\n",
        "        # Extract document-specific legal terms for highlighting\n",
        "        legal_terms = extract_legal_terms(text)\n",
        "\n",
        "        # Load the T5 summarizer model\n",
        "        tokenizer, model = load_model()\n",
        "\n",
        "        # Perform keyword-based summarization if keywords are provided\n",
        "        if keywords and keywords.strip():\n",
        "            keyword_list = [k.strip() for k in keywords.split(',')]\n",
        "            short_summary, long_summary = summarize_target_sections(\n",
        "                text, tokenizer, model, keyword_list, summary_type, num_beams\n",
        "            )\n",
        "        else:\n",
        "            # Chunk the text and generate summaries\n",
        "            chunks = chunk_text(text)\n",
        "            short_summaries, long_summaries = summarize_chunks(\n",
        "                chunks, tokenizer, model, summary_type, num_beams\n",
        "            )\n",
        "\n",
        "            # Combine summaries for coherence\n",
        "            short_summary = combine_summaries(short_summaries, SHORT_SUMMARY_LENGTH * 2)\n",
        "            long_summary = combine_summaries(long_summaries, LONG_SUMMARY_LENGTH * 2)\n",
        "\n",
        "        # Calculate summarization metrics\n",
        "        total_words = len(text.split())\n",
        "        short_word_count = len(short_summary.split())\n",
        "        long_word_count = len(long_summary.split())\n",
        "        short_compression = round((short_word_count / total_words) * 100, 2)\n",
        "        long_compression = round((long_word_count / total_words) * 100, 2)\n",
        "\n",
        "        metrics = (\n",
        "            f\"Original Word Count: {total_words}\\n\"\n",
        "            f\"Short Summary Word Count: {short_word_count} (Compression: {short_compression}%)\\n\"\n",
        "            f\"Long Summary Word Count: {long_word_count} (Compression: {long_compression}%)\"\n",
        "        )\n",
        "\n",
        "        # Create the final structured summary\n",
        "        final_summary = create_structured_summary(\n",
        "            short_summary, long_summary, metrics, legal_terms\n",
        "        )\n",
        "\n",
        "        # Save the summarized text in a temporary file for download\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".md\") as tmp_file:\n",
        "            tmp_file.write(final_summary.encode(\"utf-8\"))\n",
        "            download_path = tmp_file.name\n",
        "\n",
        "        return final_summary, download_path\n",
        "\n",
        "    except Exception as e:\n",
        "        import traceback\n",
        "        print(traceback.format_exc())\n",
        "        return f\"An error occurred: {str(e)}\", None\n",
        "\n",
        "# Gradio Interface\n",
        "def main():\n",
        "    with gr.Blocks(theme=gr.themes.Soft()) as iface:\n",
        "        gr.Markdown(\"# Legal Document Summarizer (Fine-Tuned T5)\")\n",
        "        gr.Markdown(\n",
        "            \"Upload a legal PDF to generate summaries with optional keyword-based selection. \"\n",
        "            \"The model highlights critical legal terms and provides both short and detailed summaries.\"\n",
        "        )\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                file_input = gr.File(\n",
        "                    label=\"Upload Legal Document (PDF)\",\n",
        "                    file_types=[\".pdf\"]\n",
        "                )\n",
        "                keywords_input = gr.Textbox(\n",
        "                    label=\"Target Keywords (Optional, comma-separated)\",\n",
        "                    placeholder=\"e.g., Section, Clause, Election\"\n",
        "                )\n",
        "\n",
        "                with gr.Row():\n",
        "                    summary_type = gr.Radio(\n",
        "                        [\"short\", \"long\", \"both\"],\n",
        "                        label=\"Summary Type\",\n",
        "                        value=\"both\"\n",
        "                    )\n",
        "                    beam_count = gr.Slider(\n",
        "                        1, 10, step=1, value=4,\n",
        "                        label=\"Number of Beams (Higher = more diverse)\"\n",
        "                    )\n",
        "\n",
        "                submit_btn = gr.Button(\"Generate Summary\", variant=\"primary\")\n",
        "\n",
        "            with gr.Column(scale=2):\n",
        "                output_text = gr.Markdown(label=\"Summary\")\n",
        "                download_output = gr.File(label=\"Download Summary\")\n",
        "\n",
        "        submit_btn.click(\n",
        "            fn=process_pdf,\n",
        "            inputs=[file_input, keywords_input, summary_type, beam_count],\n",
        "            outputs=[output_text, download_output]\n",
        "        )\n",
        "\n",
        "    iface.launch(debug=True, share=True)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}